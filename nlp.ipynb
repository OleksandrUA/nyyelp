{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main project\n",
    "\n",
    "#### goal:\n",
    "Predict review rating (stars) by textual-content using pretrained GLOVE word embeddings+N layers deep LSTM network. Visualize (potencial clusters) on a 2D space using t-SNE.\n",
    "\n",
    "#### Steps:  \n",
    "1) Process .text data from yelp review-documents\n",
    "- filter to leave english-only reviews\n",
    "- balance the dataset (use similar amount of data for each star rating (1,2,3,4,5*)\n",
    "- balance the dataset by review length, so the length distribution of reviews for each rating-class is atleast similar (bucketing might be a good idea)\n",
    "\n",
    "2) Process text for deep-learning\n",
    "- bring  GLOVE embeddings to a reasonable form\n",
    "- Tokenize each review \n",
    "- transform reviews into tokenized sequences\n",
    "- Pad sequences to a fixed length\n",
    "- Prepare the embedding weight-matrix to be used in the Embedding layer (we will not train this layer)\n",
    "- split into test and train\n",
    "\n",
    "\n",
    "3) Deep learning\n",
    "- loss function, regression(rmse) or classification(logloss) big question\n",
    "- prepare architecture (2-3 LSTM layers -> Dense)\n",
    "- train, wish for the best\n",
    "\n",
    "4) Post learning\n",
    "- extract features from last LSTM layer\n",
    "- try casting to 2D space using t-SNE\n",
    "\n",
    "\n",
    "5) Remarks\n",
    "- we have to limit the amount of data, bc of memory issues possible solution is to save the sequences into HDF and flowing from disk during training\n",
    "\n",
    "\n",
    "6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import langdetect\n",
    "import seaborn\n",
    "import operator\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = 'glove.6B.100d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = int(''.join([s for s in GLOVE_DIR.split('/')[-1].split('.')[-2] if s.isdigit()])) # 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "PRELOAD = False\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "# print('Indexing word vectors.')\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helpers\n",
    "\n",
    "def minority_balance_dataframe_by_multiple_categorical_variables(df, categorical_columns=None, downsample_by=0.1):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if categorical_columns is None or not all([c in df.columns for c in categorical_columns]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    minority_class_combination_count = df.groupby(categorical_columns).apply(lambda x: x.shape[0]).min()\n",
    "    \n",
    "    minority_class_combination_count = int(minority_class_combination_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(categorical_columns).apply(\n",
    "        lambda x: x.sample(minority_class_combination_count)\n",
    "    ).drop(categorical_columns, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_glove_into_dict(glove_path):\n",
    "    \"\"\"\n",
    "    loads glove file into a handy python-dict representation, where a word is a key with a corresponding N-dim vector\n",
    "    http://nlp.stanford.edu/data/glove.6B.zip (pretrained-embeddings)\n",
    "    \"\"\"\n",
    "    embeddings_ix = {}\n",
    "    with open(glove_path) as glove_file:\n",
    "        for line in glove_file:\n",
    "            val = line.split()\n",
    "            word = val[0]\n",
    "            vec = np.asarray(val[1:], dtype='float32')\n",
    "            embeddings_ix[word] = vec\n",
    "    return embeddings_ix\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "def get_features_for_layer(X, trained_model, layer_number):\n",
    "    \"\"\"\n",
    "    :param X: Batch with dimensions according to the models first layer input-shape\n",
    "    :param trained_model: Model to extract data from\n",
    "    :param layer_number: Index of the layer we want to extract features from (usually last Conv)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    get_features = K.function([trained_model.layers[0].input, K.learning_phase()],\n",
    "                              [trained_model.layers[layer_number].output])\n",
    "    \n",
    "    features = get_features([X, 0])\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171960, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1)\n",
    "\n",
    "if PRELOAD:\n",
    "    df_rev_balanced = pd.read_csv('balanced_reviews.csv')\n",
    "else:\n",
    "    df_reviews = pd.read_csv('reviews.csv')#, encoding='utf-8')\n",
    "    df_reviews['len'] = df_reviews.text.str.len()\n",
    "    df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]\n",
    "#     df_reviews = df_reviews[df_reviews.language == 'en']\n",
    "    # balancing dataset\n",
    "    df_rev_balanced = minority_balance_dataframe_by_multiple_categorical_variables(\n",
    "        df_reviews, categorical_columns=['stars'], downsample_by=0.1\n",
    "    )\n",
    "    \n",
    "    df_rev_balanced.to_csv('balanced_reviews.csv', encoding='utf-8')\n",
    "\n",
    "df_rev_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "1.0    Axes(0.125,0.125;0.775x0.775)\n",
       "2.0    Axes(0.125,0.125;0.775x0.775)\n",
       "3.0    Axes(0.125,0.125;0.775x0.775)\n",
       "4.0    Axes(0.125,0.125;0.775x0.775)\n",
       "5.0    Axes(0.125,0.125;0.775x0.775)\n",
       "Name: len, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEDCAYAAADEAyg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac2292c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#inspect review length for each star-group\n",
    "# df_rev_balanced['len'] = df_reviews.text.str.len()\n",
    "df_rev_balanced.groupby('stars')['len'].hist(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = load_glove_into_dict(GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in GLOVE: 400000\n"
     ]
    }
   ],
   "source": [
    "print('number of words in GLOVE: {}'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenization \n",
    "\n",
    "if PRELOAD:\n",
    "    tokenizer = joblib.load('tokenizer.pickle')\n",
    "else:\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df_rev_balanced.text.tolist())\n",
    "    joblib.dump(tokenizer, 'tokenizer.pickler')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('i', 3),\n",
       " ('a', 4),\n",
       " ('to', 5),\n",
       " ('was', 6),\n",
       " ('of', 7),\n",
       " ('it', 8),\n",
       " ('for', 9),\n",
       " ('in', 10)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_INDEX_SORTED[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding matrix\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(WORD_INDEX_SORTED))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seqs = tokenizer.texts_to_sequences(df_rev_balanced.text.values)\n",
    "X = pad_sequences(seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = df_rev_balanced.stars.values.astype(int)\n",
    "Y_cat = to_categorical(Y)\n",
    "assert X.shape[0] == Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_23 (Embedding)         (None, 1000, 100)     0           embedding_input_23[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "gru_14 (GRU)                     (None, 1000, 100)     60300       embedding_23[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 1000, 100)     0           gru_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "gru_15 (GRU)                     (None, 100)           60300       dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 100)           0           gru_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 6)             606         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 121206\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 137568 samples, validate on 34392 samples\n",
      "Epoch 1/40\n",
      "  7168/137568 [>.............................] - ETA: 920s - loss: 1.6485 - acc: 0.2372"
     ]
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=nb_words, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, \n",
    "                    weights=[embedding_matrix], \n",
    "                    trainable=False)\n",
    "         )\n",
    "# model.add(Convolution1D(nb_filter=128, filter_length=5, border_mode='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(GRU(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          nb_epoch=40, \n",
    "          batch_size=256, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_list\n",
    ")\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, 5, 2, 5, 2, 4, 5, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:10]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 1, 3, 2, 5, 4, 5, 5, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
